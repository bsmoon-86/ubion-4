# create a vector with all the names of the packages you want to remove혻
pkgs.to.remove <- ip[,1]
head(pkgs.to.remove)
# remove the packages
sapply(pkgs.to.remove, remove.packages, lib = path.lib)
stall.packages(c("hash", "tau", "Sejong", "RSQLite", "devtools",
"bit", "rex", "lazyeval", "htmlwidgets", "crosstalk",
"promises", "later", "sessioninfo", "xopen", "bit64",
"blob", "DBI", "mem
install.packages("remotes")
install.packages(c("hash", "tau", "Sejong", "RSQLite", "devtools",
"bit", "rex", "lazyeval", "htmlwidgets", "crosstalk",
"promises", "later", "sessioninfo", "xopen", "bit64",
"blob", "DBI", "memoise", "plogr", "covr", "DT", "rcmdcheck", "rversions"), type = "binary")
install.packages("multilinguer")
library(multilinguer)
multilinguer::install_jdk()
remotes::install_github('haven-jeon/KoNLP', upgrade = "never",
INSTALL_opts=c("--no-multiarch"))
library(KoNLP)
library(dplyr)
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
useNIADic()
library(KoNLP)
useNIADic()
extractNoun("대한민국의 영토는 한반도와 그 부속도서로 한다.")
install.packages("stringr")
library(stringr)
install.packages("stringr")
library(stringr)
txt <- readLines("hiphop.txt")
txt <- readLines("hiphop.txt")
head(txt)
txt <- str_replace_all(txt, "[[:punct:]]", " ")
# 가사에서 명사를 추출
nouns <- extractNoun(txt)
# 추출한 리스트를 문자열 백터로 변환, 단어별 빈도표 생성
wordcount <- table(unlist(nouns))
# 데이터의 형태는 데이터프레임으로 변경
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
# 컬럼의 이름을 변경
df_word <- rename(df_word,
word = Var1,
freq = Freq)
# 글자 수를 2개 이상인것만 출력
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>%
arrange(desc(freq)) %>%
head(20)
top_20
# 워드클라우드 설치
install.packages("wordcloud")
# 패키지 로드
library(wordcloud)
library(RColorBrewer)
# Dark2 색상 목록에서 8개를 추출
pal <- brewer.pal(8,"Dark2")
pal
# 난수는 랜덤하게 제작되는데 그것을 고정하는데 사용
set.seed(1234)
wordcloud(words = df_word$word,  # 단어
freq = df_word$freq,   # 빈도
min.freq = 2,          # 최소 단어 빈도
max.words = 200,       # 표현 단어의 수
random.order = F,      # 고빈도 단어의 중앙 배치
rot.per = .1,          # 회전 단어의 비율
scale = c(4, 0.3),     # 단어 크기의 범위
colors = pal)          # 색상 목록
pal <- brewer.pal(9,"Blues")[5:9]  # blue 색상 목록에서 9개를 추출하여 5번째 에서 9번째까지 추출
pal
set.seed(1234)                     # 난수 고정
wordcloud(words = df_word$word,    # 단어
freq = df_word$freq,     # 빈도
min.freq = 2,            # 최소 단어 빈도
max.words = 200,         # 표현 단어의 수
random.order = F,        # 고빈도 단어의 중앙 배치
rot.per = .1,            # 회전 단어의 비율
scale = c(4, 0.3),       # 단어 크기의 범위
colors = pal)            # 색상 목록
# create a list of all installed packages혻
ip <- as.data.frame(installed.packages())
head(ip)
# if you use MRO, make sure that no packages in this library will be removed혻
ip <- subset(ip, !grepl("MRO", ip$LibPath))
# we don't want to remove base or recommended packages either\혻
ip <- ip[!(ip[,"Priority"] %in% c("base", "recommended")),]
# determine the library where the packages are installed혻
path.lib <- unique(ip$LibPath)
# create a vector with all the names of the packages you want to remove혻
pkgs.to.remove <- ip[,1]
head(pkgs.to.remove)
# remove the packages
sapply(pkgs.to.remove, remove.packages, lib = path.lib)
#접속 주소 변수
# 환경부 국립환경과학원 미세먼지 데이터
# 필수 항목은 서비스 키
url <- "http://apis.data.go.kr/1480523/MetalMeasuringResultService/MetalService"
#서비스키
servicekey <- "dtbWOdJ%2FCz5HE0DGLU%2BCRPe7pOW0NIQBUcGEqsHZaTRiYCI%2F5%2BzugwzQjcvuId7NPdg6rUiW%2Bft3fm7yqyD4pw%3D%3D"
"
service_url <- paste0(url,
"?serviceKey=", servicekey,
"&pageNo=", pageno)
service_url
xmlDocument <- xmlTreeParse(service_url,
useInternalNodes = TRUE,
encoding = "UTF-8")
xmlDocument
## xml root node 확인
rootnode <- xmlRoot(xmlDocument)
rootnode
## 페이지당 데이터의 개수를 출력
rows <-xpathSApply(rootnode, '//numOfRows', xmlValue)
## 전체 데이터의 개수
total_rows <- xpathSApply(rootnode, '//totalCount', xmlValue)
total_rows
# 페이지당 데이터의 개수 전체데이터의 개수 나누면
# 몇번 불러와야 전체 데이터를 가지고 올수 있는지 확인 가능
# rows, total_rows 데이터가 문자형태
typeof(total_rows)
# 숫자의 형태로 변경하고 연산.
rows <- as.numeric(rows)
total_rows <- as.numeric(total_rows)
loopcount <- ceiling(total_rows/rows)
# 결과 값들은 결합할 비어있는 데이터프레임 생성
Total_data <- data.frame()
for (i in 1:10){
service_url <- paste0(url,
"?serviceKey=", servicekey,
"&pageNo=", i,
"&numOfRows=", numofrows,
"&resultType=", type_data)
## xml 형태의 데이터를 10번 반복해서 받는다.
document <- xmlTreeParse(service_url,
useInternalNodes = TRUE,
encoding= "UTF-8")
#  rootnode 확인
rootnode <- xmlRoot(document)
# item 태그안에 있는 값들을 호출
node <- getNodeSet(rootnode, '//item')
# xml데이터를 데이터프레임을 변경
df_node <- xmlToDataFrame(node)
#Total_data라는 데이터프레임에 df_node데이터프레임을 결합
Total_data <- rbind(Total_data, df_node)
}
dim(Total_data)
#서비스키
servicekey <- "dtbWOdJ%2FCz5HE0DGLU%2BCRPe7pOW0NIQBUcGEqsHZaTRiYCI%2F5%2BzugwzQjcvuId7NPdg6rUiW%2Bft3fm7yqyD4pw%3D%3D"
service_url <- paste0(url,
"?serviceKey=", servicekey,
"&pageNo=", pageno)
service_url <- paste0(url,
"?serviceKey=", servicekey)
service_url
xmlDocument <- xmlTreeParse(service_url,
useInternalNodes = TRUE,
encoding = "UTF-8")
library(XML)
xmlDocument <- xmlTreeParse(service_url,
useInternalNodes = TRUE,
encoding = "UTF-8")
xmlDocument
## xml root node 확인
rootnode <- xmlRoot(xmlDocument)
rootnode
## 페이지당 데이터의 개수를 출력
rows <-xpathSApply(rootnode, '//numOfRows', xmlValue)
## 전체 데이터의 개수
total_rows <- xpathSApply(rootnode, '//totalCount', xmlValue)
total_rows
# 페이지당 데이터의 개수 전체데이터의 개수 나누면
# 몇번 불러와야 전체 데이터를 가지고 올수 있는지 확인 가능
# rows, total_rows 데이터가 문자형태
typeof(total_rows)
# 숫자의 형태로 변경하고 연산.
rows <- as.numeric(rows)
total_rows <- as.numeric(total_rows)
loopcount <- ceiling(total_rows/rows)
loopcount
# 결과 값들은 결합할 비어있는 데이터프레임 생성
Total_data <- data.frame()
for (i in 1:10){
service_url <- paste0(url,
"?serviceKey=", servicekey,
"&pageNo=", i,
"&numOfRows=", numofrows,
"&resultType=", type_data)
## xml 형태의 데이터를 10번 반복해서 받는다.
document <- xmlTreeParse(service_url,
useInternalNodes = TRUE,
encoding= "UTF-8")
#  rootnode 확인
rootnode <- xmlRoot(document)
# item 태그안에 있는 값들을 호출
node <- getNodeSet(rootnode, '//item')
# xml데이터를 데이터프레임을 변경
df_node <- xmlToDataFrame(node)
#Total_data라는 데이터프레임에 df_node데이터프레임을 결합
Total_data <- rbind(Total_data, df_node)
}
for (i in 1:10){
service_url <- paste0(url,
"?serviceKey=", servicekey,
"&pageNo=", i)
## xml 형태의 데이터를 10번 반복해서 받는다.
document <- xmlTreeParse(service_url,
useInternalNodes = TRUE,
encoding= "UTF-8")
#  rootnode 확인
rootnode <- xmlRoot(document)
# item 태그안에 있는 값들을 호출
node <- getNodeSet(rootnode, '//item')
# xml데이터를 데이터프레임을 변경
df_node <- xmlToDataFrame(node)
#Total_data라는 데이터프레임에 df_node데이터프레임을 결합
Total_data <- rbind(Total_data, df_node)
}
dim(Total_data)
View(Total_data)
write.csv(Total_data, "미세먼지.csv")
write.csv(Total_data, "미세먼지.csv", row.names = F)
library(XML)
#접속 주소 변수
url <- "http://apis.data.go.kr/1160100/service/GetFnCoBasiInfoService/getFnCoOutl"
#서비스키
servicekey <- "dtbWOdJ%2FCz5HE0DGLU%2BCRPe7pOW0NIQBUcGEqsHZaTRiYCI%2F5%2BzugwzQjcvuId7NPdg6rUiW%2Bft3fm7yqyD4pw%3D%3D"
#페이지넘버
pageno <- 1
#페이지당 데이터의 개수
numofrows <- 10
#데이터의 타입
type_data <- "xml"
service_url <- paste0(url,
"?serviceKey=", servicekey,
"&pageNo=", pageno,
"&numOfRows=", numofrows,
"&resultType=", type_data)
service_url
xmlDocument <- xmlTreeParse(service_url,
useInternalNodes = TRUE,
encoding = "UTF-8")
xmlDocument
## xml root node 확인
rootnode <- xmlRoot(xmlDocument)
rootnode
## 페이지당 데이터의 개수를 출력
rows <-xpathSApply(rootnode, '//numOfRows', xmlValue)
## 전체 데이터의 개수
total_rows <- xpathSApply(rootnode, '//totalCount', xmlValue)
total_rows
# 페이지당 데이터의 개수 전체데이터의 개수 나누면
# 몇번 불러와야 전체 데이터를 가지고 올수 있는지 확인 가능
# rows, total_rows 데이터가 문자형태
typeof(total_rows)
# 숫자의 형태로 변경하고 연산.
rows <- as.numeric(rows)
total_rows <- as.numeric(total_rows)
loopcount <- ceiling(total_rows/rows)
# 결과 값들은 결합할 비어있는 데이터프레임 생성
Total_data <- data.frame()
for (i in 1:2){
service_url <- paste0(url,
"?serviceKey=", servicekey,
"&pageNo=", i,
"&numOfRows=", numofrows,
"&resultType=", type_data)
## xml 형태의 데이터를 10번 반복해서 받는다.
document <- xmlTreeParse(service_url,
useInternalNodes = TRUE,
encoding= "UTF-8")
#  rootnode 확인
rootnode <- xmlRoot(document)
# item 태그안에 있는 값들을 호출
node <- getNodeSet(rootnode, '//item')
# xml데이터를 데이터프레임을 변경
df_node <- xmlToDataFrame(node)
#Total_data라는 데이터프레임에 df_node데이터프레임을 결합
Total_data <- rbind(Total_data, df_node)
}
dim(Total_data)
View(Total_data)
## 데이터프레임을 csv로 만들기
## write.csv(데이터프레임명, 파일의이름, 옵션....)
## row.names 옵션은 csv에 인덱스 값을 넣어줄지 지정
write.csv(Total_data, "금융.csv", row.names = F)
# 접속 주소 변수
# 환경부 국립환경과학원 미세먼지 데이터
# 필수 항목은 서비스 키
url <- "http://apis.data.go.kr/1480523/MetalMeasuringResultService/MetalService"
#서비스키
servicekey <- "dtbWOdJ%2FCz5HE0DGLU%2BCRPe7pOW0NIQBUcGEqsHZaTRiYCI%2F5%2BzugwzQjcvuId7NPdg6rUiW%2Bft3fm7yqyD4pw%3D%3D"
service_url <- paste0(url,
"?serviceKey=", servicekey)
service_url
xmlDocument <- xmlTreeParse(service_url,
useInternalNodes = TRUE,
encoding = "UTF-8")
xmlDocument <- xmlTreeParse(service_url,
useInternalNodes = TRUE,
encoding = "UTF-8")
xmlDocument
## xml root node 확인
rootnode <- xmlRoot(xmlDocument)
rootnode
## 페이지당 데이터의 개수를 출력
rows <-xpathSApply(rootnode, '//numOfRows', xmlValue)
## 전체 데이터의 개수
total_rows <- xpathSApply(rootnode, '//totalCount', xmlValue)
total_rows
# 숫자의 형태로 변경하고 연산.
rows <- as.numeric(rows)
total_rows <- as.numeric(total_rows)
loopcount <- ceiling(total_rows/rows)
loopcount
for (i in 1:2){
service_url <- paste0(url,
"?serviceKey=", servicekey,
"&pageNo=", i)
## xml 형태의 데이터를 10번 반복해서 받는다.
document <- xmlTreeParse(service_url,
useInternalNodes = TRUE,
encoding= "UTF-8")
#  rootnode 확인
rootnode <- xmlRoot(document)
# item 태그안에 있는 값들을 호출
node <- getNodeSet(rootnode, '//item')
# xml데이터를 데이터프레임을 변경
df_node <- xmlToDataFrame(node)
#Total_data라는 데이터프레임에 df_node데이터프레임을 결합
Total_data <- rbind(Total_data, df_node)
}
# 접속 주소 변수
# 환경부 국립환경과학원 미세먼지 데이터
# 필수 항목은 서비스 키
url <- "http://apis.data.go.kr/1480523/MetalMeasuringResultService/MetalService"
#서비스키
servicekey <- "dtbWOdJ%2FCz5HE0DGLU%2BCRPe7pOW0NIQBUcGEqsHZaTRiYCI%2F5%2BzugwzQjcvuId7NPdg6rUiW%2Bft3fm7yqyD4pw%3D%3D"
service_url <- paste0(url,
"?serviceKey=", servicekey)
service_url
xmlDocument <- xmlTreeParse(service_url,
useInternalNodes = TRUE,
encoding = "UTF-8")
xmlDocument
## xml root node 확인
rootnode <- xmlRoot(xmlDocument)
rootnode
## 페이지당 데이터의 개수를 출력
rows <-xpathSApply(rootnode, '//numOfRows', xmlValue)
## 전체 데이터의 개수
total_rows <- xpathSApply(rootnode, '//totalCount', xmlValue)
total_rows
# 페이지당 데이터의 개수 전체데이터의 개수 나누면
# 몇번 불러와야 전체 데이터를 가지고 올수 있는지 확인 가능
# rows, total_rows 데이터가 문자형태
typeof(total_rows)
# 숫자의 형태로 변경하고 연산.
rows <- as.numeric(rows)
total_rows <- as.numeric(total_rows)
loopcount <- ceiling(total_rows/rows)
loopcount
# 결과 값들은 결합할 비어있는 데이터프레임 생성
Total_data <- data.frame()
for (i in 1:2){
service_url <- paste0(url,
"?serviceKey=", servicekey,
"&pageNo=", i)
## xml 형태의 데이터를 10번 반복해서 받는다.
document <- xmlTreeParse(service_url,
useInternalNodes = TRUE,
encoding= "UTF-8")
#  rootnode 확인
rootnode <- xmlRoot(document)
# item 태그안에 있는 값들을 호출
node <- getNodeSet(rootnode, '//item')
# xml데이터를 데이터프레임을 변경
df_node <- xmlToDataFrame(node)
#Total_data라는 데이터프레임에 df_node데이터프레임을 결합
Total_data <- rbind(Total_data, df_node)
}
dim(Total_data)
View(Total_data)
write.csv(Total_data, "미세먼지.csv", row.names = F)
write.csv(Total_data, "미세먼지.csv", row.names = F)
write.csv(Total_data, "미세먼지.csv", row.names = T)
write.csv(Total_data, "미세먼지.csv", row.names = F)
install.packages(c("hash", "tau", "Sejong", "RSQLite", "devtools",
"bit", "rex", "lazyeval", "htmlwidgets", "crosstalk",
"promises", "later", "sessioninfo", "xopen", "bit64",
"blob", "DBI", "memoise", "plogr", "covr", "DT", "rcmdcheck", "rversions"), type = "binary")
install.packages("multilinguer")
library(multilinguer)
multilinguer::install_jdk()
library(multilinguer)
multilinguer::install_jdk()
install.packages("remotes")
remotes::install_github('haven-jeon/KoNLP', upgrade = "never",
INSTALL_opts=c("--no-multiarch"))
library(KoNLP)
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
useNIADic()
extractNoun("조회날짜, 항목코드, 측정소코드, 시간구분을 기준으로 중금속 성분 측정 결과를 2시간 평균, 24시간 평균 측정 수치 자료로 제공하는 서비스")
twitter <- read.csv("twitter.csv", herder=T,
fileEncoding = "UTF-8")
twitter <- read.csv("twitter.csv", header=T,
fileEncoding = "UTF-8")
twitter <- rename(twitter,
no = 번호,
id = 계정이름,
date = 작성일,
tw = 내용)
head(twitter)
## 특수문자 제거
install.packages("stringr")
install.packages("stringr")
library(stringr)
twitter <- str_replace_all(twitter, "[[:punct:]]", " ")
#twitter <- str_replace_all(twitter, "[[:punct:]]", " ")
twitter <- str_replace_all(twitter, "\\W", " ")
nouns <- extractNoun(twitter$tw)
twitter$tw
View(twitter)
twitter <- read.csv("twitter.csv", header=T,
fileEncoding = "UTF-8")
twitter <- rename(twitter,
no = 번호,
id = 계정이름,
date = 작성일,
tw = 내용)
#twitter <- str_replace_all(twitter, "[[:punct:]]", " ")
twitter$tw <- str_replace_all(twitter$tw, "\\W", " ")
nouns <- extractNoun(twitter$tw)
nouns
typeof(nouns)
wordcount <- table(unlist(nouns))
wordcount
df_word <- as.data.frame(wordcount)
View(df_word)
df_word <- rename(df_word,
word = Var1,
freq = Freq)
## nchar()글자의 수를 출력
## 글자의 수가 2개 이상인 경우만 보여주겠다
df_word <- filter(df_word, nchar(word) >= 2)
typeof(df_word$word)
## word 컬럼의 값이 숫자형에서 문자형 변경
df_word$word <- as.character(df_word$word)
## nchar()글자의 수를 출력
## 글자의 수가 2개 이상인 경우만 보여주겠다
df_word <- filter(df_word, nchar(word) >= 2)
## 워드클라우드 설치
install.packages("wordcloud")
## 워드클라우드 로드
library(RColorBrewer)
library(wordcloud)
## Dark2 색상목록 이중에서 8개를 출력
pal <- brewer.pal(8, "Dark2")
pal
# 난수를 랜덤하게 생성. 시드를 지정하면 랜덤이 고정
set.seed(1234)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
random.order = F,
max.words = 200,
rot.per = .1,
scale = c(4, 0.3),
color = pal)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
random.order = F,
max.words = 200,
rot.per = .1,
scale = c(6, 1),
color = pal)
set.seed(1234)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
random.order = F,
max.words = 200,
rot.per = .1,
scale = c(6, 1),
color = pal)
set.seed(1234)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
random.order = F,
max.words = 200,
rot.per = .1,
scale = c(6, 1),
color = pal)
set.seed(1234)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
random.order = F,
max.words = 200,
rot.per = .1,
scale = c(4, 0.5),
color = pal)
## XML 데이터를 수정을 하기 위한 패키지
#install.packages("XML")
library(XML)
